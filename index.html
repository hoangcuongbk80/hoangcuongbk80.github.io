<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name=viewport content="width=800">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
      color: #1772d0;
      text-decoration: none;
    }
    
    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }
    
    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px
    }
    
    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
    }
    
    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 22px;
    }
    
    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
      font-weight: 700
    }
    
    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 32px;
    }
    
    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }
    
    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
	  
    
    span.highlight {
      background-color: #ffffd0;
    }
  </style>
  <link rel="icon" type="image/png" href="images/seal_icon.png">
  <title>Dinh-Cuong Hoang</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
</head>

<body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="67%" valign="middle">
              <p align="center">
                <name>Dinh-Cuong Hoang</name>
              </p>
              <p> 
                I am a Researcher and Lecturer at <a href="https://uni.fpt.edu.vn/en-US/home">FPT University</a> where I work on Computer Vision, Artificial Intelligence (AI) and Robotics. I received my Ph.D. degree in Computer Science at <a href="https://www.oru.se/english/">Orebro University, Sweden</a> and was advised by <a href="https://scholar.google.ca/citations?user=GJIaXm4AAAAJ&hl=en">Prof. Todor Stoyanov</a>, and <a href="https://scholar.google.com/citations?user=_CdZ5cgAAAAJ&hl=en">Prof. Achim J. Lilienthal</a>. Previously, I did my bachelors at the <a href="https://en.hust.edu.vn/">Hanoi University of Science and Technology (HUST)</a>.
              </p>
			  
	      <p>
          My research interests lie at the intersection of computer vision, robotics, and machine learning. I am particularly interested in topics involving w ebsite autonomy for robots, with a focus on perception algorithms.		      
	      </p> 

        <p>
          Teaching courses: Computer Architecture; Programming Fundamentals; Object-Oriented Programming; Data Structures and Algorithms; Digital Image Processing; Advanced Computer Vision; Programming for Robotics; Robotics: Perception; Introduction of Deep Learning.		      
	      </p>

        <p>
          Projects: <a href="https://iliad-project.eu/">ILIAD</a>, <a href="https://www.youtube.com/watch?v=61RKefFBRzo&feature=youtu.be">Sub-OBB</a>, <a href="https://www.youtube.com/watch?v=bAdFVLzUHoU&feature=youtu.be"> Vision 4.0</a>
	      </p>
			  
              <p align=center>
		<a href="http://oru.diva-portal.org/smash/get/diva2:1605796/FULLTEXT01.pdf">Ph.D. Thesis</a> &nbsp/&nbsp
                <a href="data/HoangCuong-CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=OBZVQ48AAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
		<a href="https://github.com/hoangcuongbk80/">GitHub</a> &nbsp/&nbsp
		<a href="https://www.linkedin.com/in/dinh-cuong-hoang-343a4469/">LinkedIn</a> &nbsp/&nbsp
              </p>
            </td>
            <td width="33%">
              <img src="images/cuong.png">
            </td>
          </tr>
	      
	  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading>News</heading>
		    <p>
		    <span>&#x25cf;</span> &nbsp; <b>[Fe. 2022]</b> &nbsp; I serve as a Reviewer for European Conference on Computer Vision 2022 (<a href="https://eccv2022.ecva.net/">ECCV2022</a>) and the 25th International Conference on Medical Image
Computing and Computer Assisted Intervention (<a href="https://conferences.miccai.org/2022/en/">MICCAI2022</a>)
		    </p>
		    
		    <p>
			<span>&#x25cf;</span> &nbsp; <b>[Dec. 2021]</b> &nbsp; Our research team got $220K funding from <a href="https://vinif.org/">Vingroup Innovation Foundation - VinIF</a> to enhance medication safety. 
		    	</p>
		    
            </td>
          </tr>
        </table>
	       
	      
	<table width="95%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>	  
	<tr onmouseout="loss_stop()" onmouseover="loss_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='loss_image'><img src='images/Sensors_2019.png'></div>
                <img src='images/Sensors_2019.png'>
              </div>
              <script type="text/javascript">
                function loss_start() {
                  document.getElementById('loss_image').style.opacity = "1";
                }
                function loss_stop() {
                  document.getElementById('loss_image').style.opacity = "0";
                }
                loss_stop()
              </script>		    
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.ncbi.nlm.nih.gov/pubmed/31022945">
                <papertitle>Spatio-Temporal Image Representation of 3D Skeletal Movements for View-Invariant Action Recognition with Deep Convolutional Neural Networks</a>
	      <br> 
	      <font color="red"><strong>Journal Paper</strong></font>    
              <br>
              <strong>Huy Hieu Pham</strong>, 
	      <a href="https://scholar.google.fr/citations?user=yvLitLEAAAAJ&hl=fr">Houssam Salmane</a>,
	      <a href="https://www.researchgate.net/profile/Louahdi_Khoudour">Louahdi Khoudour</a>,
	      <a href="https://www.irit.fr/~Alain.Crouzil/">Alain Crouzil</a>,
	      <a href="https://www.researchgate.net/profile/Pablo_Zegers">Pablo Zegers</a>, and
	      <a href="https://www.uc3m.es/ss/Satellite/UC3MInstitucional/en/FormularioTextoDosColumnas/1371216239282/Very_experienced_fellow">Sergio A Velastin</a>
              <br>
              <em>Deep Learning-Based Image Sensors, Intelligent Sensors</em>, Sensors 2019
              <br>
              <a href="https://www.preprints.org/manuscript/201903.0086/v1">Preprint</a> /
	      <a href=" https://doi.org/10.3390/s19081932">DOI</a> /    
              <a href="data/HieuPham_Sensors_2019.bib">BibTeX</a>
	       <p></p>
              <p> A new motion representation called Enhanced-SPMF for human action recognition in videos with deep neural networks.</p>
            </td>
          </tr>
	  </tr>
        </table>  
	 
	<table width="95%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
	<tr onmouseout="loss_stop()" onmouseover="loss_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='loss_image'><img src='images/IET_2018.png'></div>
                <img src='images/IET_2018.png'>
              </div>
              <script type="text/javascript">
                function loss_start() {
                  document.getElementById('loss_image').style.opacity = "1";
                }
                function loss_stop() {
                  document.getElementById('loss_image').style.opacity = "0";
                }
                loss_stop()
              </script>		    
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://digital-library.theiet.org/content/journals/10.1049/iet-cvi.2018.5014">
                <papertitle>Learning to Recognize 3D Human Action from A New Skeleton-based Representation Using Deep Convolutional Neural Networks</papertitle>
              </a>
	      <br> 
	      <font color="red"><strong>Journal Paper</strong></font>    
              <br>
              <strong>Huy Hieu Pham</strong>, 
	      <a href="https://www.researchgate.net/profile/Louahdi_Khoudour">Louahdi Khoudour</a>,
	      <a href="https://www.irit.fr/~Alain.Crouzil/">Alain Crouzil</a>,
	      <a href="https://www.researchgate.net/profile/Pablo_Zegers">Pablo Zegers</a>, and
	      <a href="https://www.uc3m.es/ss/Satellite/UC3MInstitucional/en/FormularioTextoDosColumnas/1371216239282/Very_experienced_fellow">Sergio A Velastin</a>
              <br>
              <em>IET Computer Vision</em>, IET 2018
              <br>
              <a href="https://arxiv.org/pdf/1812.10550.pdf">arXiv</a> /
	      <a href="https://doi.org/10.1049/iet-cvi.2018.5014">DOI</a> /    
              <a href="data/HieuPham_IET_2018.bib">BibTeX</a>
              <p></p>
              <p>Transforming 3D joint coordinates of the human body carried in skeleton sequences into RGB images for human action recognition in videos with deep neural networks.</p>
            </td>
          </tr>
	  </tr>
        </table>  
	  
	<table width="95%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
	<tr onmouseout="loss_stop()" onmouseover="loss_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='loss_image'><img src='images/ICIP_2018.png'></div>
                <img src='images/ICIP_2018.png'>
              </div>
              <script type="text/javascript">
                function loss_start() {
                  document.getElementById('loss_image').style.opacity = "1";
                }
                function loss_stop() {
                  document.getElementById('loss_image').style.opacity = "0";
                }
                loss_stop()
              </script>		    
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/8451404">
                <papertitle>Skeletal Movement to Color Map: A Novel Representation for 3D Action Recognition with Inception Residual Networks</papertitle>
              </a>
	      <br> 
	      <font color="red"><strong>Conference Paper</strong></font>    
              <br>
              <strong>Huy Hieu Pham</strong>, 
	      <a href="https://www.researchgate.net/profile/Louahdi_Khoudour">Louahdi Khoudour</a>,
	      <a href="https://www.irit.fr/~Alain.Crouzil/">Alain Crouzil</a>,
	      <a href="https://www.researchgate.net/profile/Pablo_Zegers">Pablo Zegers</a>, and
	      <a href="https://www.uc3m.es/ss/Satellite/UC3MInstitucional/en/FormularioTextoDosColumnas/1371216239282/Very_experienced_fellow">Sergio A Velastin</a>
              <br>
              <em>The 25th IEEE International Conference on Image Processing</em>, ICIP2018, October 7-10, 2018, Athens, Greece
              <br>
              <a href="https://arxiv.org/pdf/1807.07033.pdf">arXiv</a> /
	      <a href="https://doi.org/10.1109/ICIP.2018.8451404">DOI</a> /    
              <a href="data/HieuPham_ICIP_2018.bib">BibTeX</a>
	      <p></p>
              <p>Proposing a new 3D skeleton-based representation, namely, SPMF (Skeleton Pose-Motion Feature) for video-based human action recogntion with depth sensors.</p>
            </td>
          </tr>
	  </tr>
        </table>  
		  
	<table width="95%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
	<tr onmouseout="loss_stop()" onmouseover="loss_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='loss_image'><img src='images/CVIU_figure.png'></div>
                <img src='images/CVIU_figure.png'>
              </div>
              <script type="text/javascript">
                function loss_start() {
                  document.getElementById('loss_image').style.opacity = "1";
                }
                function loss_stop() {
                  document.getElementById('loss_image').style.opacity = "0";
                }
                loss_stop()
              </script>		    
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.sciencedirect.com/science/article/pii/S1077314218300389">
                <papertitle>Exploiting Deep Residual Networks for Human Action Recognition from Skeletal Data</papertitle>
              </a>
	      <br> 
	      <font color="red"><strong>Journal Paper</strong></font>    
              <br>
              <strong>Huy Hieu Pham</strong>, 
	      <a href="https://www.researchgate.net/profile/Louahdi_Khoudour">Louahdi Khoudour</a>,
	      <a href="https://www.irit.fr/~Alain.Crouzil/">Alain Crouzil</a>,
	      <a href="https://www.researchgate.net/profile/Pablo_Zegers">Pablo Zegers</a>, and
	      <a href="https://www.uc3m.es/ss/Satellite/UC3MInstitucional/en/FormularioTextoDosColumnas/1371216239282/Very_experienced_fellow">Sergio A Velastin</a>
              <br>
              <em>Computer Vision and Image Understanding</em>, CVIU 2018
              <br>
              <a href="https://arxiv.org/pdf/1803.07781.pdf">arXiv</a> /
	      <a href="https://doi.org/10.1016/j.cviu.2018.03.003">DOI</a> /    
	      <a href="https://github.com/huyhieupham/Improved-ResNet-Action-Recognition-Skeletal-Data">Code</a> /
              <a href="data/HieuPham_CVIU_2018.bib">BibTeX</a>
	      <p></p>
              <p>Investigating and applying deep ResNets for human action recognition using skeletal data provided by depth sensors.</p>
            </td>
          </tr>
	  </tr>
        </table>  
		  
	<table width="95%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
	<tr onmouseout="loss_stop()" onmouseover="loss_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='loss_image'><img src='images/DefautJENv1.png'></div>
                <img src='images/DefautJENv1.png'>
              </div>
              <script type="text/javascript">
                function loss_start() {
                  document.getElementById('loss_image').style.opacity = "1";
                }
                function loss_stop() {
                  document.getElementById('loss_image').style.opacity = "0";
                }
                loss_stop()
              </script>		    
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://link.springer.com/article/10.1007/s10921-017-0453-1">
                <papertitle>3D Point Cloud Analysis for Detection and Characterization of Defects on Airplane Exterior Surface</papertitle>
              </a>
	      <br> 
	      <font color="red"><strong>Journal Paper</strong></font>    
              <br>
              Igor Jovančević, <strong>Huy Hieu Pham</strong>, Jean-José Orteu, Rémi Gilblas, Jacques Harvent, Xavier Maurice, and Ludovic Brèthes
              <br>
              <em>Journal of Nondestructive Evaluation</em>, JNE 2017
              <br>
              <a href="https://hal-mines-albi.archives-ouvertes.fr/hal-01622056/document">Full-text PDF</a> /
	      <a href="https://doi.org/10.1007/s10921-017-0453-1">DOI</a> /    
              <a href="data/Jovancevic2017JNE.bib">BibTeX</a>
              <p></p>
              <p>A novel automatic vision-based inspection system that is capable of detecting and characterizing defects on an airplane exterior surface.</p>
            </td>
          </tr>
	  </tr>
        </table>  
		
	<table width="95%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr> 	  
	   <tr onmouseout="loss_stop()" onmouseover="loss_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='loss_image'><img src='images/Nuage.png'></div>
                <img src='images/Nuage.png'>
              </div>
              <script type="text/javascript">
                function loss_start() {
                  document.getElementById('loss_image').style.opacity = "1";
                }
                function loss_stop() {
                  document.getElementById('loss_image').style.opacity = "0";
                }
                loss_stop()
              </script>		    
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://hal.archives-ouvertes.fr/hal-01660998/">
                <papertitle>Détection et Caractérisation de Défauts de Surface par Analyse des Nuages de Points 3D Fournis par un Scanner</papertitle>
              </a>
	      <br> 
	      <font color="red"><strong>Journal Paper</strong></font>    
              <br>
              Igor Jovančević, <strong>Huy Hieu Pham</strong>, Jean-José Orteu, Rémi Gilblas, Jacques Harvent, Xavier Maurice, and Ludovic Brèthes
              <br>
              <em>Journal of Sensors</em>, 2016
              <br>
              <a href="https://hal.archives-ouvertes.fr/hal-01660998/document">Full-text PDF</a> /
              <a href="data/jovancevic:hal2017.bib">BibTeX</a>
              <p></p>
              <p>A system that is able to detect obstacles in indoor environment based on Kinect sensor and 3D-image processing.</p>
            </td>
          </tr>
	  </tr>
        </table>
	  
	<table width="95%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr> 
	<tr onmouseout="loss_stop()" onmouseover="loss_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='loss_image'><img src='images/ResNet.png'></div>
                <img src='images/Kinect-Joint.png'>
              </div>
              <script type="text/javascript">
                function loss_start() {
                  document.getElementById('loss_image').style.opacity = "1";
                }
                function loss_stop() {
                  document.getElementById('loss_image').style.opacity = "0";
                }
                loss_stop()
              </script>		    
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://digital-library.theiet.org/content/conferences/10.1049/cp.2017.0154">
                <papertitle>Learning and Recognizing Human Action from Skeleton Movement with Deep Residual Neural Networks</papertitle>
              </a>
	      <br> 
	      <font color="red"><strong>Conference Paper</strong></font>
              <br>
              <strong>Huy Hieu Pham</strong>, 
	      <a href="https://www.researchgate.net/profile/Louahdi_Khoudour">Louahdi Khoudour</a>,
	      <a href="https://www.irit.fr/~Alain.Crouzil/">Alain Crouzil</a>,
	      <a href="https://www.researchgate.net/profile/Pablo_Zegers">Pablo Zegers</a>, and
	      <a href="https://www.uc3m.es/ss/Satellite/UC3MInstitucional/en/FormularioTextoDosColumnas/1371216239282/Very_experienced_fellow">Sergio A Velastin</a>
              <br>
              <em>The 8th International Conference of Pattern Recognition Systems</em>, ICPRS2017, July 12-13, 2017, Madrid, Spain
              <br>
              <a href="https://arxiv.org/pdf/1803.07780.pdf">arXiv</a> /
	      <a href="https://doi.org/10.1049/cp.2017.0154">DOI</a> /
              <a href="data/HieuPhamCVPRS2017.bib">BibTeX</a>
              <p></p>
              <p>Training Deep Residual Neural Networks to learn and recognize human action from skeleton data provided by Kinect sensor.</p>
            </td>
          </tr>
	   </tr>
        </table>
	  
	<table width="95%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>	
	<tr onmouseout="loss_stop()" onmouseover="loss_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='loss_image'><img src='images/obtacle1.png'></div>
                <img src='images/obtacle2.png'>
              </div>
              <script type="text/javascript">
                function loss_start() {
                  document.getElementById('loss_image').style.opacity = "1";
                }
                function loss_stop() {
                  document.getElementById('loss_image').style.opacity = "0";
                }
                loss_stop()
              </script>		    
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.hindawi.com/journals/js/2016/3754918/abs/">
                <papertitle>Real-time Obstacle Detection System in Indoor Environment for the Visually Impaired Using Microsoft Kinect Sensor</papertitle>
              </a>
	      <br> 
	      <font color="red"><strong>Journal Paper</strong></font>
              <br>
              <strong>Huy Hieu Pham</strong>, 
	      <a href="https://www.mica.edu.vn/perso/Le-Thi-Lan/">Thi Lan Le</a>, and
	      <a href="https://www.omicsonline.org/editor-profile/Nicolas_Vuillerme/">Nicolas Vuillerme</a>
              <br>
              <em>Journal of Sensors</em>, 2016 
		    
              <br>
              <a href="http://downloads.hindawi.com/journals/js/2016/3754918.pdf">Full-text PDF</a> /
              <a href="https://www.hindawi.com/journals/js/2016/3754918/">Full-text HTML </a> /
	      <a href="http://dx.doi.org/10.1155/2016/3754918/>DOI</a> /
              <a href="https://www.youtube.com/watch?v=9IQPptJbO4M">Video</a> /
              <a href="data/HieuPhamSensor2016.bib">BibTeX</a>
              <p></p>
              <p>A system that is able to detect obstacles in indoor environment based on Kinect sensor and 3D-image processing.</p>
            </td>
          </tr>
						   
          </tr>
		
						   
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Teaching</heading>
					    
		<p>    
		   <span>&#x25cf;</span> &nbsp;  <b>Instructor</b>, College of Engineering & Computer Science (CECS), VinUniversity. <br>
				&nbsp; &nbsp; &nbsp; <b> Course:</b> <i> "Introduction to Programing - CECS-COMP1010 (Fall 2021-2022)"</i>, with Prof. <a href="https://vinuni.edu.vn/people/kok-seng-wong/">Kok-Seng Wong</a>
		</p>					    
				
					    
					    
		<p>    
		   <span>&#x25cf;</span> &nbsp;  <b>Instructor</b>, College of Engineering & Computer Science (CECS), VinUniversity. <br>
				&nbsp; &nbsp; &nbsp; <b> Course:</b> <i> "Introduction to Machine Learning - CECS-1020 (Spring 2021-2022)"</i>, with Prof. <a href="https://vinuni.edu.vn/people/minh-do/">Minh Do</a>
		</p>					    
					    
		<p>    
		   <span>&#x25cf;</span> &nbsp;  <b>Guest Lecturer</b>, University Carlos III of Madrid, Spain. <br>
		                  &nbsp; &nbsp; &nbsp;  <b> Course:</b> 	<i>"An introduction to Deep Learning for Image and Video Interpretation"</i>, (Fall 2017-2018).
		</p>						    
					    
		<p>    
		   <span>&#x25cf;</span> &nbsp;  <b>Teaching Assistant</b>, University of Toulouse, France. <br>
					&nbsp; &nbsp; &nbsp;  <b> Course:</b> <i> "Introduction to Programming and Algorithms in Python"</i>, (Fall 2018-2019).															
		</p>						    
            </td>
          </tr>
	 </table>						   
						   
			   
						   
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Professional Activities</heading>
		<p>    
		   <span>&#x25cf;</span> &nbsp;  <b>Reviewer</b>,  European Conference on Computer Vision (<a href="https://eccv2022.ecva.net/">ECCV 2022</a>) 
		</p>
					    				    
		<p>    
		   <span>&#x25cf;</span> &nbsp;  <b>Reviewer</b>,  International Conference on Medical Image Computing and Computer Assisted Intervention (<a href="https://conferences.miccai.org/2022/en/">MICCAI 2022</a>) 
		</p>
					    
		<p>    
		   <span>&#x25cf;</span> &nbsp;  <b>Reviewer</b>, IEEE Journal of Biomedical and Health Informatics (<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6221020">JBHI</a>) 
		</p>
					    
					    
					    
		<p>    
		   <span>&#x25cf;</span> &nbsp;  <b>Reviewer</b>, Journal of Electronic Imaging (<a href="https://www.spiedigitallibrary.org/journals/journal-of-electronic-imaging?SSO=1">JEI</a>) 
		</p>					    
					    
		<p>    
		   <span>&#x25cf;</span> &nbsp;  <b>Reviewer</b>, IET Computer Vision Journal (<a href="https://digital-library.theiet.org/content/journals/iet-cvi">IET-CVI</a>)
		</p>						    
					    
		<p>    
		   <span>&#x25cf;</span> &nbsp;  <b>Reviewer</b>, International Conference on Computer Vision (<a href="http://iccv2021.thecvf.com/home">ICCV 2021</a>)
		</p>
			
		<p>    
		   <span>&#x25cf;</span> &nbsp;  <b>Reviewer</b>, IEEE/CVF Conference on Computer Vision and Pattern Recognition 2022 (<a href="https://cvpr2022.thecvf.com/">CVPR2022</a>) 
		</p>			

		<p>    
		   <span>&#x25cf;</span> &nbsp;  <b>Reviewer</b>, Nature <a href="https://www.nature.com/srep/">Scientific Reports</a>
		</p>
		
		<p>    
		   <span>&#x25cf;</span> &nbsp;  <b>Program Commitee</b>, International Conference on Multimedia Analysis and Pattern Recognition (<a href="https://mapr.uit.edu.vn/program-committee">MAPR</a>)	   
		</p>
					    
					    			    
					    
					    
            </td>
          </tr>
	 </table>					   
						   
					    
					    
					   			    
					    
        </table>
	      
        <table width="100%" align="center" border="0" cellpadding="20"> 
	
            
              <p>
                
                <br>
                <br>
                
                <br>
                <br>
              </p>
            </td>
        
								      

								      
							      
								      
								      
								      
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <br>
              <p align="right">
                <font size="2">
                  Thanks <a href="https://jonbarron.info/">Jon Barron</a> for his awesome open source code.
       
                  </font>
              </p>
            </td>
          </tr>
        </table>	
	      

		
        
         <!–– Please delete this script if you use this HTML. ––>
        <script type="text/javascript">
          var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
          document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
        </script>
        <script type="text/javascript">
          try {
            var pageTracker = _gat._getTracker("UA-7580334-1");
            pageTracker._trackPageview();
          } catch (err) {}
        </script>
        </td>
    </tr> 
  </table>
</body>
</html>		
				     
							     
